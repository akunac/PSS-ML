{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning for Power System Stability Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:azure;padding:10px;border:2px solid lightsteelblue\"><b>Author:</b> Petar Sarajcev, PhD (petar.sarajcev@fesb.hr)\n",
    "<br>\n",
    "University of Split, FESB, Department of Power Engineering <br>R. Boskovica 32, HR-21000 Split, Croatia, EU.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inline figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure aesthetics\n",
    "sns.set(context='notebook', style='white', font_scale=1.2)\n",
    "sns.set_style('ticks', {'xtick.direction':'in', 'ytick.direction':'in'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ancilary function from: https://github.com/amueller/introduction_to_ml_with_python/blob/master/mglearn/tools.py\n",
    "def heatmap(values, xlabel, ylabel, xticklabels, yticklabels, cmap=None,\n",
    "            vmin=None, vmax=None, ax=None, fmt=\"%0.2f\", fontsize=14):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # plot the mean cross-validation scores\n",
    "    img = ax.pcolor(values, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    img.update_scalarmappable()\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xticks(np.arange(len(xticklabels)) + .5)\n",
    "    ax.set_yticks(np.arange(len(yticklabels)) + .5)\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    for p, color, value in zip(img.get_paths(), img.get_facecolors(),\n",
    "                               img.get_array()):\n",
    "        x, y = p.vertices[:-2, :].mean(0)\n",
    "        if np.mean(color[:3]) > 0.5:\n",
    "            c = 'k'\n",
    "        else:\n",
    "            c = 'w'\n",
    "        ax.text(x, y, fmt % value, color=c, ha=\"center\", va=\"center\", fontsize=fontsize)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer diagnostic data and health index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('GridDictionary.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip ones into zeros for the \"Stability\" column\n",
    "#data['Stability'] = 1 - data['Stability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of \"ones\" in the \"Stability\" column\n",
    "print('There is {:.1f}% of unstable cases in the dataset!'.format(data['Stability'].sum()/float(len(data['Stability']))*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a random subset of the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random subset of the original dataset (without replacement)\n",
    "SUBSET_SIZE = 2000\n",
    "random_idx = np.random.choice(data.index, size=SUBSET_SIZE, replace=False)\n",
    "data = data.iloc[random_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing and splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "no_features = len(data.columns) - 1\n",
    "X_data = data.iloc[:,0:no_features]  # features\n",
    "print('X_data', X_data.shape)\n",
    "y_data = data['Stability']\n",
    "print('y_data', y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, train_size=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train', X_train.shape)\n",
    "print('y_train', y_train.shape)\n",
    "print('X_test', X_test.shape)\n",
    "print('y_test', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t = data[['Stability']].copy()\n",
    "idx = y_test.index.values\n",
    "y_t = y_t.loc[idx]\n",
    "y_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the input data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression (with fixed hyper-parameters)\n",
    "lreg = LR(C=100.,  # fixed \"C\" hyper-parameter\n",
    "          multi_class='ovr', solver='newton-cg', n_jobs=-1)\n",
    "lreg.fit(X_train, y_train)  # fit model to data\n",
    "y_lr = lreg.predict_proba(X_test)  # predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lreg.predict(X_test)\n",
    "labels = ['Stab', 'NotStab']\n",
    "# confusion matrix\n",
    "scores_image = heatmap(metrics.confusion_matrix(y_test, pred), xlabel='Predicted label', \n",
    "                       ylabel='True label', xticklabels=labels, yticklabels=labels, \n",
    "                       cmap=plt.cm.gray_r, fmt=\"%d\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "print(metrics.classification_report(y_test, pred, target_names=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid-search with cross validation for optimal model hyper-parameters\n",
    "parameters = {'C':[1., 10., 50., 100., 500., 1000.]}\n",
    "lreg = GridSearchCV(estimator=LR(multi_class='ovr', solver='newton-cg'), \n",
    "                    param_grid=parameters, cv=3, scoring='f1',  # notice the \"scoring\" method!\n",
    "                    refit=True, n_jobs=-1, iid=False)\n",
    "lreg.fit(X_train, y_train)\n",
    "# Best value of hyper-parameter \"C\"\n",
    "best_c = lreg.best_params_['C']\n",
    "print('Best value: C = {:g}'.format(best_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average classification accuracy with cross validation\n",
    "scores = cross_val_score(LR(C=best_c, multi_class='ovr', solver='newton-cg'), \n",
    "                         X_train, y_train, cv=3)  # it doesn't return a model!\n",
    "print('Score using 3-fold CV: {:g} +/- {:g}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection with Pipeline and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the number of features and the classifier's hyper-parameters \n",
    "# at the same time, using pipline and grid search with cross-validation\n",
    "pca = PCA()  # do NOT set \"n_components\" here!\n",
    "logreg = LR(multi_class='ovr', solver='newton-cg')  # multinomial classification!\n",
    "pipe = Pipeline([('pca',pca), ('logreg',logreg)])\n",
    "param_grid = {'pca__n_components': [10, 50, 100],  # PCA\n",
    "              'logreg__C': [10., 100., 500.]}      # LogisticRegression\n",
    "grid_pipe = GridSearchCV(estimator=pipe, param_grid=param_grid, cv=3, \n",
    "                         scoring='f1', refit=True, n_jobs=-1, iid=False)\n",
    "grid_pipe.fit(X_train, y_train)\n",
    "print('Best parameter (CV score = {:0.3f}):'.format(grid_pipe.best_score_))\n",
    "print(grid_pipe.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probability on test data\n",
    "y_lr = grid_pipe.predict_proba(X_test)\n",
    "y_t['logreg'] = y_lr.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters ={'C':[1., 10., 100., 500., 1000.],\n",
    "             'gamma':[0.0001, 0.001, 0.01, 0.1, 1.]}\n",
    "svc = GridSearchCV(estimator=svm.SVC(kernel='rbf', probability=True), \n",
    "                   param_grid=parameters, cv=3,\n",
    "                   scoring='f1', refit=True, n_jobs=-1, iid=False)\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model parameters\n",
    "best_parameters = svc.best_params_\n",
    "print(\"Best parameters from GridSearch: {}\".format(svc.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(svm.SVC(**best_parameters), X_train, y_train, cv=3)\n",
    "print('Average score using 3-fold CV: {:g} +/- {:g}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(svc.cv_results_)\n",
    "scores = np.array(results.mean_test_score).reshape(len(parameters['C']), len(parameters['gamma']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "heatmap(scores, xlabel='gamma', xticklabels=parameters['gamma'], \n",
    "        ylabel='C', yticklabels=parameters['C'], cmap=\"viridis\", ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C':stats.expon(scale=100), 'gamma':stats.expon(scale=.1)}\n",
    "svc2 = RandomizedSearchCV(estimator=svm.SVC(kernel='rbf', probability=True), \n",
    "                          param_distributions=parameters, cv=3, n_iter=50,  # 50 iterations!\n",
    "                          scoring='neg_log_loss',  # notice the scoring method!\n",
    "                          refit=True, n_jobs=-1, iid=False)\n",
    "svc2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model parameters\n",
    "best_parameters = svc2.best_params_\n",
    "print(\"Best parameters from RandomSearch: {}\".format(svc2.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(svm.SVC(**best_parameters), X_train, y_train, cv=3)\n",
    "print('Average score using 3-fold CV: {:g} +/- {:g}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_svc2 = svc2.predict_proba(X_test)\n",
    "y_t['svc'] = y_svc2.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision-Recall Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = cross_val_predict(svm.SVC(**best_parameters, probability=True), \n",
    "                             X_train, y_train, cv=3, method='predict_proba')\n",
    "y_scores = y_probas[:,1]  # score = probability of positive class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions, recalls, thresholds = metrics.precision_recall_curve(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.set_title('SVC Precision-Recall tradeof')\n",
    "ax.plot(thresholds, precisions[:-1], lw=2, label='Precision')\n",
    "ax.plot(thresholds, recalls[:-1], lw=2, label='Recall')\n",
    "plt.vlines(0.5, 0, 1, linestyles='--', label='Threshold = 0.5')\n",
    "ax.set_xlabel('Thresholds')\n",
    "ax.legend(loc='best')\n",
    "ax.set_ylim(ymin=0.8, ymax=1.02)\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4.5,4.5))\n",
    "ax.plot(precisions, recalls, lw=2, label='SVC')\n",
    "default = np.argmin(np.abs(thresholds - 0.5))\n",
    "ax.plot(precisions[default], recalls[default], '^', c='k', markersize=10, \n",
    "        label='Threshold = 0.5', fillstyle='none', mew=2)\n",
    "ax.set_xlabel('Precision')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.legend(loc='best')\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average precision-recall score\n",
    "y_test_score = svc2.predict_proba(X_test)[:,1]\n",
    "average_precision = metrics.average_precision_score(y_test, y_test_score)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine a class from the predicted probability by using \n",
    "# the user-specified threshold value (not a default of 0.5)\n",
    "THRESHOLD = 0.6\n",
    "preds = np.where(y_test_score > THRESHOLD, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data=[metrics.accuracy_score(y_test, preds), metrics.recall_score(y_test, preds),\n",
    "                   metrics.precision_score(y_test, preds), metrics.roc_auc_score(y_test, preds)], \n",
    "             index=[\"accuracy\", \"recall\", \"precision\", \"roc_auc_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ExtraTreesClassifier (ensemble learner) with grid search \n",
    "# and cross-validation for hyper-parameters optimisation\n",
    "parameters = {'n_estimators':[5, 10, 15, 20], \n",
    "              'criterion':['gini', 'entropy'], \n",
    "              'max_depth':[2, 5, None]}\n",
    "trees = GridSearchCV(estimator=ExtraTreesClassifier(), param_grid=parameters, \n",
    "                     cv=3, scoring='neg_log_loss', refit=True, n_jobs=-1, iid=False) \n",
    "trees.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model parameters\n",
    "best_parameters = trees.best_params_\n",
    "print(\"Best parameters: {}\".format(trees.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(ExtraTreesClassifier(**best_parameters), X_train, y_train, cv=3)\n",
    "print('Average score using 3-fold CV: {:g} +/- {:g}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trees = trees.predict_proba(X_test)\n",
    "y_t['tree'] = y_trees.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest classifier (ensemble learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier (ensemble learner for classification)\n",
    "parameters = {'n_estimators':[10, 15, 20], \n",
    "              'criterion':['gini', 'entropy'],\n",
    "              'max_features':[4, 'auto'],\n",
    "              'max_depth':[2, None]}\n",
    "# grid search and cross-validation for hyper-parameters optimisation\n",
    "forest = GridSearchCV(estimator=RandomForestClassifier(), param_grid=parameters, \n",
    "                      cv=3, scoring='neg_log_loss', refit=True, n_jobs=-1, iid=False) \n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = forest.best_params_\n",
    "print(\"Best parameters: {}\".format(forest.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(RandomForestClassifier(**best_parameters), X_train, y_train, cv=3)\n",
    "print('Average score using 3-fold CV: {:g} +/- {:g}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_forest = forest.predict_proba(X_test)\n",
    "y_t['forest'] = y_forest.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoosting classifier with feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & evaluate model performance\n",
    "def train_and_evaluate(model, X, y, ns=3):\n",
    "    # k-fold cross validation iterator \n",
    "    cv = KFold(n_splits=ns, shuffle=True)\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='f1')  # scoring method is f1!\n",
    "    print('Average score using {:d}-fold CV: {:g} +/- {:g}'.format(ns, np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Classifier\n",
    "clf_gb = GradientBoostingClassifier()\n",
    "train_and_evaluate(clf_gb, X_train, y_train, 3)\n",
    "clf_gb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = clf_gb.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top features\n",
    "TOP = 10\n",
    "print('Most relevant {:d} features according to the GradientBoostingClassifier:'.format(TOP))\n",
    "data.columns.values[sorted_idx][-TOP:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot relative feature importance\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.barh(pos[-TOP:], feature_importance[sorted_idx][-TOP:], align='center', color='magenta', alpha=0.6)\n",
    "plt.yticks(pos[-TOP:], data.columns[sorted_idx][-TOP:])\n",
    "ax.set_xlabel('Feature Relative Importance')\n",
    "#ax.grid(which='major', axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix of selected features\n",
    "pearson = data[data.columns[sorted_idx][-TOP:]].corr('pearson')\n",
    "pearson.iloc[-1][:-1].sort_values()\n",
    "# Correlation matrix as heatmap (seaborn)\n",
    "fig, ax = plt.subplots(figsize=(6.5,5.5))\n",
    "sns.heatmap(pearson, annot=True, annot_kws=dict(size=9), vmin=-1, vmax=1, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on new data\n",
    "y_gb = clf_gb.predict_proba(X_test)\n",
    "y_t['gbr'] = y_gb.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble models using voting principle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:honeydew;padding:10px;border:2px solid mediumseagreen\"><b>Note:</b> Ensembling consists of pooling together the predictions of a set of different models, to produce better predictions. The key to making ensembling work is the diversity of the set of classifiers. Diversity is what makes ensembling work. For this reason, one should ensemble models that are as good as possible while being <b>as different as possible</b>. This typically means using very different network architectures or even different brands of machine-learning approaches. This is exactly what has been proposed here.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = VotingClassifier(estimators=[('logreg', lreg),     # LogisticRegression\n",
    "                                   ('svm', svc2),        # SVC\n",
    "                                   ('forest', forest)],  # RandomForest \n",
    "                       weights=[1, 1, 1],  # classifier relative weights\n",
    "                       voting='soft')\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_clf = clf.predict_proba(X_test)\n",
    "y_t['vote'] = y_clf.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(clf, X_train, y_train, cv=3)\n",
    "print('Average score using 3-fold CV: {:g} +/- {:g}'.format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions using individual classifiers and ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:honeydew;padding:10px;border:2px solid mediumseagreen\"><b>Note:</b> Reported model accuracy depends on the random synthetic dataset used during the learning phase, which has been generated from the original dataset (used for testing) by means of the simple \"data augmentation\" technique. Possibility for overfitting and underfitting should be further examined, preferably with a larger dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, IPython, platform, sklearn, scipy\n",
    "print(\"Notebook createad on {:s} computer running {:s} and using:\\\n",
    "      \\nPython {:s}\\nIPython {:s}\\nScikit-learn {:s}\\nPandas {:s}\\nNumpy {:s}\\nScipy {:s}\"\\\n",
    "      .format(platform.machine(), ' '.join(platform.linux_distribution()[:2]), sys.version[:5], \n",
    "              IPython.__version__, sklearn.__version__, pd.__version__, np.__version__, scipy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
